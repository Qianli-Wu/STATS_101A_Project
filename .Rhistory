data <- read.csv("inverter.csv")
inverter <- read.csv("inverter.csv")
View(inverter)
inverter
inverter.model <- lm(y~x1 + x2 + x3 + x4 + x5, data=inverter)
anova(inverter.model)
summary(inverter.model)
inverter.model <- lm(y~x1 + x2 + x3 + x4 + x5, data=inverter)
summary(inverter.model)
# delete x5 from the model
inverter.new.model <- lm(y~x1 + x2 + x3 + x4, data=inverter)
summary(inverter.new.model)
install.packages("relaimpo")
MSE1 <- mean(summary(inverter.model)$residuals^2)
MSE2 <- mean(summary(inverter.new.model)$residuals^2)
cat("MSE for part a is: ", MSE1, "\nMSE for part c isL ", MSE2)
cat("MSE for part a is: ", MSE1, "\nMSE for part c is: ", MSE2)
install.packages("MASS")
library(MASS)
studres(inverter.new.model)
max(studres(inverter.new.model))
max(abs(studres(inverter.new.model)))
inverter <- inverter[-2,]
su <- summary(inverter.new.model)
View(su)
# delete x5 from the model
inverter.new.model <- lm(y~x1 + x2 + x3 + x4, data=inverter)
summary(inverter.new.model)
inverter <- inverter[-2,]
old.r <- summary(inverter.new.model)$r.squared
inverter.new.model <- lm(y~x1 + x2 + x3 + x4, data=inverter)
inverter <- inverter[-2,]
old.r <- summary(inverter.new.model)$r.squared
inverter.new.model <- lm(y~x1 + x2 + x3 + x4, data=inverter)
new.r <- summary(inverter.new.model)$r.squared
cat("Old Multiple R square is: ", old.r, "\nNew Multiple R square is: ", new.r,)
inverter <- inverter[-2,]
old.r <- summary(inverter.new.model)$r.squared
inverter.new.model <- lm(y~x1 + x2 + x3 + x4, data=inverter)
new.r <- summary(inverter.new.model)$r.squared
cat("Old Multiple R square is: ", old.r, "\nNew Multiple R square is: ", new.r)
knitr::opts_chunk$set(echo = TRUE)
inverter <- read.csv("inverter.csv")
inverter
inverter.model <- lm(y~x1 + x2 + x3 + x4 + x5, data=inverter)
summary(inverter.model)
# delete x5 from the model
inverter.new.model <- lm(y~x1 + x2 + x3 + x4, data=inverter)
summary(inverter.new.model)
MSE1 <- mean(summary(inverter.model)$residuals^2)
MSE2 <- mean(summary(inverter.new.model)$residuals^2)
cat("MSE for part a is: ", MSE1, "\nMSE for part c is: ", MSE2)
library(MASS)
studres(inverter.new.model)
max(abs(studres(inverter.new.model)))
inverter <- inverter[-2,]
old.r <- summary(inverter.new.model)$r.squared
inverter.new.model <- lm(y~x1 + x2 + x3 + x4, data=inverter)
new.r <- summary(inverter.new.model)$r.squared
cat("Old Multiple R square is: ", old.r, "\nNew Multiple R square is: ", new.r)
summary(inverter.new.model)
plot(inverter.new.model)
library(relaimpo)
calc.relimp(inverter.model)
library(relaimpo)
calc.relimp(inverter.model)
library(relaimpo)
contributions <- calc.relimp(inverter.model)
View(contributions)
contribution$lmg
library(relaimpo)
contributions <- calc.relimp(inverter.model)
contribution$lmg
library(relaimpo)
contributions <- calc.relimp(inverter.model)
contribution$lmg
library(relaimpo)
contributions <- calc.relimp(inverter.model)
contribution
library(relaimpo)
contributions <- calc.relimp(inverter.model)
contributions$lmg
# delete x5 from the model
inverter.new.model <- lm(y~x1 + x2 + x3 + x4, data=inverter)
summary(inverter.new.model)
# delete x5 from the model
inverter.new.model <- lm(y~x1 + x2 + x3 + x4, data=inverter)
summary(inverter.new.model)
calc.relimp(inverter.new.model)$lmg
MSE1 <- mean(summary(inverter.model)$residuals^2)
MSE2 <- mean(summary(inverter.new.model)$residuals^2)
cat("MSE for part a is: ", MSE1, "\nMSE for part c is: ", MSE2)
knitr::opts_chunk$set(echo = TRUE)
inverter <- read.csv("inverter.csv")
inverter
inverter.model <- lm(y~x1 + x2 + x3 + x4 + x5, data=inverter)
summary(inverter.model)
library(relaimpo)
contributions <- calc.relimp(inverter.model)
contributions$lmg
# delete x5 from the model
inverter.new.model <- lm(y~x1 + x2 + x3 + x4, data=inverter)
summary(inverter.new.model)
MSE1 <- mean(summary(inverter.model)$residuals^2)
MSE2 <- mean(summary(inverter.new.model)$residuals^2)
cat("MSE for part a is: ", MSE1, "\nMSE for part c is: ", MSE2)
library(MASS)
studres(inverter.new.model)
max(abs(studres(inverter.new.model)))
inverter <- inverter[-2,]
old.r <- summary(inverter.new.model)$r.squared
inverter.new.model <- lm(y~x1 + x2 + x3 + x4, data=inverter)
new.r <- summary(inverter.new.model)$r.squared
cat("Old Multiple R square is: ", old.r, "\nNew Multiple R square is: ", new.r)
summary(inverter.new.model)
plot(inverter.new.model)
hist(studres(inverter.new.model))
hist(abs(studres(inverter.new.model)))
knitr::opts_chunk$set(echo = TRUE)
inverter <- read.csv("inverter.csv")
inverter
inverter.model <- lm(y~x1 + x2 + x3 + x4 + x5, data=inverter)
summary(inverter.model)
library(relaimpo)
contributions <- calc.relimp(inverter.model)
contributions$lmg
# delete x5 from the model
inverter.new.model <- lm(y~x1 + x2 + x3 + x4, data=inverter)
summary(inverter.new.model)
MSE1 <- mean(summary(inverter.model)$residuals^2)
MSE2 <- mean(summary(inverter.new.model)$residuals^2)
cat("MSE for part a is: ", MSE1, "\nMSE for part c is: ", MSE2)
library(MASS)
studres(inverter.new.model)
max(abs(studres(inverter.new.model)))
hist(abs(studres(inverter.new.model)))
inverter <- inverter[-2,]
old.r <- summary(inverter.new.model)$r.squared
inverter.new.model <- lm(y~x1 + x2 + x3 + x4, data=inverter)
new.r <- summary(inverter.new.model)$r.squared
cat("Old Multiple R square is: ", old.r, "\nNew Multiple R square is: ", new.r)
summary(inverter.new.model)
plot(inverter.new.model)
inverter.sum <- summary(inverter.new.model)
inverter.sum
inverter.sum$residuals
res <- inverter.sum$residuals
y_hat <- inverter.sum$fstatistic
y_hat <- inverter.sum$fitted.values
res <- inverter.sum$residuals
y_hat <- inverter.sum$fitted.values
res <- inverter.sum$residuals
y_hat <- inverter.sum$fitted.values
plot(y_hat, res, xlab = 'Fitted values', ylab = 'Residuals',
main = 'Residuals vs Fitted')
inverter.sum <- summary(inverter.new.model)
inverter.sum
View(inverter.new.model)
View(inverter.sum)
inverter_sum <- summary(inverter.new.model)
inverter_sum
View(inverter.new.model)
res <- inverter.sum$residuals
y_hat <- inverter.new.model$fitted.values
plot(y_hat, res, xlab = 'Fitted values', ylab = 'Residuals',
main = 'Residuals vs Fitted')
abline(h = 0, lty = 'dashed')
inverter.sum <- summary(inverter.new.model)
inverter.sum
res <- inverter.sum$residuals
y_hat <- inverter.new.model$fitted.values
plot(y_hat, res, xlab = 'Fitted values', ylab = 'Residuals',
main = 'Residuals vs Fitted')
abline(h = 0, lty = 'dashed')
x1 <- inverter$x1
plot(x1, res, xlab = 'x1', ylab = 'Residuals',
main = 'Residuals vs x1')
abline(h = 0, lty = 'dashed')
colnames(inverter)
for predictor in colnames(inverter)[2:6]{
for predictor in colnames(inverter)[2:6]{
for (predictor in colnames(inverter)[2:6]) {
predict_value <- inverter$predictor
plot(predict_value, res, xlab = predict, ylab = 'Residuals',
main = paste('Residuals vs ', predict))
abline(h = 0, lty = 'dashed')
}
for (predictor in colnames(inverter)[2:6]) {
print(predictor)
predict_value <- inverter$predictor
plot(predict_value, res, xlab = predict, ylab = 'Residuals',
main = paste('Residuals vs ', predict))
abline(h = 0, lty = 'dashed')
}
for (predictor in colnames(inverter)[2:6]) {
predict_value <- inverter$predictor
print(predict_value)
plot(predict_value, res, xlab = predict, ylab = 'Residuals',
main = paste('Residuals vs ', predict))
abline(h = 0, lty = 'dashed')
}
for (predictor in colnames(inverter)[2:6]) {
predict_value <- inverter[predictor]
plot(predict_value, res, xlab = predict, ylab = 'Residuals',
main = paste('Residuals vs ', predict))
abline(h = 0, lty = 'dashed')
}
for (predictor in colnames(inverter)[2:6]) {
predict_value <- inverter[predictor]
print(predict_value)
plot(predict_value, res, xlab = predict, ylab = 'Residuals',
main = paste('Residuals vs ', predict))
abline(h = 0, lty = 'dashed')
}
x1 <- inverter$x1
plot(x1, res, xlab = predict, ylab = 'Residuals',
main = paste('Residuals vs ', predict))
x1 <- inverter$x1
plot(x1, res, xlab = predict, ylab = 'Residuals',
main = 'Residuals vs x1')
x1 <- inverter$x1
plot(x1, res, xlab = 'x1', ylab = 'Residuals',
main = 'Residuals vs x1')
abline(h = 0, lty = 'dashed')
x2 <- inverter$x2
plot(x2, res, xlab = 'x2', ylab = 'Residuals',
main = 'Residuals vs x2')
abline(h = 0, lty = 'dashed')
x3 <- inverter$x3
plot(x3, res, xlab = 'x3', ylab = 'Residuals',
main = 'Residuals vs x3')
abline(h = 0, lty = 'dashed')
x4 <- inverter$x4
plot(x4, res, xlab = 'x4', ylab = 'Residuals',
main = 'Residuals vs x4')
abline(h = 0, lty = 'dashed')
knitr::opts_chunk$set(echo = TRUE)
# import data
lol.data <- read.csv("lol_games.csv")
# remove the column of game id and game duration
lol.data <- lol.data[,-1]
lol.data <- lol.data[,-1]
lol.model <- lm(goldDiff~., data = lol.data)
summary(lol.model)
# Since in summary of lol.model, we have all NA in these four predictors
# Thus, inspect the four columns
cat("Range of destroyedTopBaseTurret: ", range(lol.data$destroyedTopBaseTurret),
"\nRange of destroyedMidBaseTurret: ", range(lol.data$destroyedMidBaseTurret),
"\nRange of lostTopBaseTurret: ", range(lol.data$lostTopBaseTurret),
"\nRange of lostMidBaseTurret: ", range(lol.data$lostMidBaseTurret))
# Since all values in these four columns are 0, we can remove these predictors
# import tidyverse for Data Cleaning
library(tidyverse)
# If the Base Turret is not destroyed, then the Inhibitor cannot be destroyed
# Thus, the observations with Top/Mid Inhibitor > 0 but Top/Mid BaseTurret are invalid
# Therefore, we remove these observations and then remove these columns
# Also, BaseTurrent cannot be destroyed twice
# Thus, the observations with BotBaseTurret > 2 are invalid
# We Also need to remove these
lol.data <- lol.data %>%
# remove the observations with more than 1 Bottom Base Turrent being destroyed
filter(destroyedBotBaseTurret < 2) %>%
filter(lostBotBaseTurret < 2) %>%
# remove the observations with destroyed Top inhibitor but not Top Base Turret
filter(destroyedTopInhibitor == 0) %>%
filter(lostTopInhibitor == 0) %>%
# remove the observations with destroyed Mid inhibitor but not Mid Base Turret
filter(destroyedMidInhibitor == 0) %>%
filter(lostMidInhibitor == 0) %>%
# remove the variables with all 0s
select(!c(destroyedTopBaseTurret, destroyedMidBaseTurret, lostTopBaseTurret, lostMidBaseTurret, lostMidInhibitor, destroyedMidInhibitor, lostTopInhibitor, destroyedTopInhibitor))
# Preview the dataframe
as.tibble(lol.data)
lol.model <- lm(goldDiff~., data = lol.data)
summary(lol.model)
# Check multi-chollinearity
library(corrplot)
# Visualize the aliasing in the model matrix, excluding the intercept.
X <- model.matrix(~.-1, data = lol.data[,-1])
# Create color map on pairwise correlations.
contrast.vectors.correlations <- cor(X)
corrplot(contrast.vectors.correlations, type = "full", addgrid.col = "gray",
tl.col = "black", tl.srt = 90, method = "color", tl.cex=0.3)
# eliminate predictors
library(leaps)
Best_Subset <- regsubsets(goldDiff ~., data = lol.data,
nbest = 1, nvmax=NULL,
force.in = NULL, force.out=NULL,
method = "forward",
really.big=T)
summary_best_subset <- summary(Best_Subset)
# as.data.frame(summary_best_subset$outmat)
# let’s see what the package recommends in terms of the number of predictors to use for our dataset
which.max(summary_best_subset$adjr2)
# Summary table below provides details on which predictors to use for the model.
# The best predictors are indicated by ‘TRUE’.
# Select all predictors (remove the intercept)
predictor.list <- summary_best_subset$which[which.max(summary_best_subset$adjr2),][-1]
# Only keep the predictors are indicated by ‘TRUE’ and our response 'goldDiff'
keep <- names(predictor.list[predictor.list==T])
keep <- append(keep, c("goldDiff"))
# Using 'keep' as a mask to select best predictors in data set
new.lol.data <- lol.data[,(names(lol.data) %in% keep)]
# Preview the selected data set with only best predictors
as.tibble(new.lol.data)
lol.new.model <- lm(goldDiff~., data=new.lol.data)
summary(lol.new.model)
anova(lol.model, lol.new.model)
# Visualize the aliasing in the model matrix, excluding the intercept.
X <- model.matrix(~.-1, data = new.lol.data[,-1])
# Create color map on pairwise correlations.
contrast.vectors.correlations <- cor(X)
corrplot(contrast.vectors.correlations, type = "full", addgrid.col = "gray",
tl.col = "black", tl.srt = 90, method = "color", tl.cex=0.4)
library(car)
vif_values <- vif(lol.new.model)
vif_values
par(oma=c(0,1,0,0))
#create horizontal bar chart to display each VIF value
barplot(vif_values, main = "VIF Values", horiz = TRUE, col = "cyan", las=1, cex.names=0.4)
#add vertical line at 5
abline(v = 5, lwd = 3, lty = 2)
p <- plot(lol.new.model)
avPlot(lol.new.model, "kills")
drop <- c("expDiff")
new.lol.data[,!(names(new.lol.data) %in% drop)]
as.tibble(new.lol.data)
drop <- c("expDiff")
new.lol.data <- new.lol.data[,!(names(new.lol.data) %in% drop)]
as.tibble(new.lol.data)
lol.new.model <- lm(goldDiff~., data=new.lol.data)
summary(lol.new.model)
avPlot(lol.new.model, "assists")
# residual analysis -> identify bad high leverage points and outliers
p <- length(new.lol.data)
n <- length(new.lol.data$goldDiff)
high_leverage <- 2*p/n
plot(no_water_drake)
# residual analysis -> identify bad high leverage points and outliers
p <- length(new.lol.data)
n <- length(new.lol.data$goldDiff)
high_leverage <- 2*p/n
plot(lol.new.model)
abline(a =4, b=0)
abline(a =-4, b=0)
abline(v = high_leverage)
# residual analysis -> identify bad high leverage points and outliers
p <- length(new.lol.data)
n <- length(new.lol.data$goldDiff)
high_leverage <- 2*p/n
plot(lol.new.model)
abline(a =4, b=0, col = "red")
abline(a =-4, b=0, col = "red")
abline(v = high_leverage, col = "blue")
# residual analysis -> identify bad high leverage points and outliers
p <- length(new.lol.data)
n <- length(new.lol.data$goldDiff)
high_leverage <- 2*p/n
plot(lol.new.model)
abline(a =4, b=0, col = "red", lty = 2)
abline(a =-4, b=0, col = "red")
abline(v = high_leverage, col = "blue")
# residual analysis -> identify bad high leverage points and outliers
p <- length(new.lol.data)
n <- length(new.lol.data$goldDiff)
high_leverage <- 2*p/n
plot(lol.new.model)
abline(a =4, b=0, col = "red", lwd = 2)
abline(a =-4, b=0, col = "red", lwd = 2)
abline(v = high_leverage, col = "blue", lwd = 2)
avPlot(lol.new.model, "assists")
avPlot(lol.new.model, "wardsPlaced")
avPlot(lol.new.model, "lostWaterDrake")
avPlot(lol.new.model, "lostEarthDrake")
avPlot(lol.new.model, "killedFireDrake")
# Plots for potetial 'bad' predictors
avPlot(lol.new.model, "assists")
par(mfrow=c(2,2))
avPlot(lol.new.model, "wardsPlaced")
avPlot(lol.new.model, "lostWaterDrake")
avPlot(lol.new.model, "lostEarthDrake")
avPlot(lol.new.model, "killedFireDrake")
par(mfrow=c(2,2))
avPlots(lol.new.model)
knitr::opts_chunk$set(echo = TRUE)
# import data
lol.data <- read.csv("lol_games.csv")
# remove the column of game id and game duration
lol.data <- lol.data[,-1]
lol.data <- lol.data[,-1]
lol.model <- lm(goldDiff~., data = lol.data)
summary(lol.model)
# Since in summary of lol.model, we have all NA in these four predictors
# Thus, inspect the four columns
cat("Range of destroyedTopBaseTurret: ", range(lol.data$destroyedTopBaseTurret),
"\nRange of destroyedMidBaseTurret: ", range(lol.data$destroyedMidBaseTurret),
"\nRange of lostTopBaseTurret: ", range(lol.data$lostTopBaseTurret),
"\nRange of lostMidBaseTurret: ", range(lol.data$lostMidBaseTurret))
# Since all values in these four columns are 0, we can remove these predictors
# import tidyverse for Data Cleaning
library(tidyverse)
# If the Base Turret is not destroyed, then the Inhibitor cannot be destroyed
# Thus, the observations with Top/Mid Inhibitor > 0 but Top/Mid BaseTurret are invalid
# Therefore, we remove these observations and then remove these columns
# Also, BaseTurrent cannot be destroyed twice
# Thus, the observations with BotBaseTurret > 2 are invalid
# We Also need to remove these
lol.data <- lol.data %>%
# remove the observations with more than 1 Bottom Base Turrent being destroyed
filter(destroyedBotBaseTurret < 2) %>%
filter(lostBotBaseTurret < 2) %>%
# remove the observations with destroyed Top inhibitor but not Top Base Turret
filter(destroyedTopInhibitor == 0) %>%
filter(lostTopInhibitor == 0) %>%
# remove the observations with destroyed Mid inhibitor but not Mid Base Turret
filter(destroyedMidInhibitor == 0) %>%
filter(lostMidInhibitor == 0) %>%
# remove the variables with all 0s
select(!c(destroyedTopBaseTurret, destroyedMidBaseTurret, lostTopBaseTurret, lostMidBaseTurret, lostMidInhibitor, destroyedMidInhibitor, lostTopInhibitor, destroyedTopInhibitor))
# Preview the dataframe
as.tibble(lol.data)
lol.model <- lm(goldDiff~., data = lol.data)
summary(lol.model)
# Check multi-chollinearity
library(corrplot)
# Visualize the aliasing in the model matrix, excluding the intercept.
X <- model.matrix(~.-1, data = lol.data[,-1])
# Create color map on pairwise correlations.
contrast.vectors.correlations <- cor(X)
corrplot(contrast.vectors.correlations, type = "full", addgrid.col = "gray",
tl.col = "black", tl.srt = 90, method = "color", tl.cex=0.3)
# eliminate predictors
library(leaps)
Best_Subset <- regsubsets(goldDiff ~., data = lol.data,
nbest = 1, nvmax=NULL,
force.in = NULL, force.out=NULL,
method = "forward",
really.big=T)
summary_best_subset <- summary(Best_Subset)
# as.data.frame(summary_best_subset$outmat)
# let’s see what the package recommends in terms of the number of predictors to use for our dataset
which.max(summary_best_subset$adjr2)
# Summary table below provides details on which predictors to use for the model.
# The best predictors are indicated by ‘TRUE’.
# Select all predictors (remove the intercept)
predictor.list <- summary_best_subset$which[which.max(summary_best_subset$adjr2),][-1]
# Only keep the predictors are indicated by ‘TRUE’ and our response 'goldDiff'
keep <- names(predictor.list[predictor.list==T])
keep <- append(keep, c("goldDiff"))
# Using 'keep' as a mask to select best predictors in data set
new.lol.data <- lol.data[,(names(lol.data) %in% keep)]
# Preview the selected data set with only best predictors
as.tibble(new.lol.data)
lol.new.model <- lm(goldDiff~., data=new.lol.data)
summary(lol.new.model)
anova(lol.model, lol.new.model)
# Visualize the aliasing in the model matrix, excluding the intercept.
X <- model.matrix(~.-1, data = new.lol.data[,-1])
# Create color map on pairwise correlations.
contrast.vectors.correlations <- cor(X)
corrplot(contrast.vectors.correlations, type = "full", addgrid.col = "gray",
tl.col = "black", tl.srt = 90, method = "color", tl.cex=0.4)
# Using 'vif' function in 'car' package
library(car)
vif_values <- vif(lol.new.model)
# Set the left margin of plot be 1
par(oma=c(0,1,0,0))
#create horizontal bar chart to display each VIF value
barplot(vif_values, main = "VIF Values", horiz = TRUE, col = "cyan", las=1, cex.names=0.4)
#add vertical line at 5
abline(v = 5, lwd = 3, lty = 2)
# remove expDiff
drop <- c("expDiff")
new.lol.data <- new.lol.data[,!(names(new.lol.data) %in% drop)]
as.tibble(new.lol.data)
lol.new.model <- lm(goldDiff~., data=new.lol.data)
summary(lol.new.model)
# Using 'vif' function in 'car' package
library(car)
vif_values <- vif(lol.new.model)
# Set the left margin of plot be 1
par(oma=c(0,1,0,0))
#create horizontal bar chart to display each VIF value
barplot(vif_values, main = "VIF Values", horiz = TRUE, col = "cyan", las=1, cex.names=0.4)
#add vertical line at 5
abline(v = 5, lwd = 3, lty = 2)
# residual analysis -> identify bad high leverage points and outliers
p <- length(new.lol.data)
n <- length(new.lol.data$goldDiff)
high_leverage <- 2*p/n
# Display four plot at a time
par(mfrow=c(2,2 ))
plot(lol.new.model)
abline(a =4, b=0, col = "red", lwd = 2)
abline(a =-4, b=0, col = "red", lwd = 2)
abline(v = high_leverage, col = "blue", lwd = 2)
# Plots for potetial 'bad' predictors
avPlot(lol.new.model, "assists")
par(mfrow=c(2,2))
avPlot(lol.new.model, "wardsPlaced")
avPlot(lol.new.model, "lostWaterDrake")
avPlot(lol.new.model, "lostEarthDrake")
avPlot(lol.new.model, "killedFireDrake")
# Plots for some 'significant' predictors
par(mfrow=c(2,2))
avPlot(lol.new.model, c("champLevelDiff", "kills"))
# Plots for some 'significant' predictors
par(mfrow=c(2,2))
avPlot(lol.new.model, "champLevelDiff")
avPlot(lol.new.model, "destroyedBotBaseTurret")
avPlot(lol.new.model, "lostTopOuterTurret")
avPlot(lol.new.model, "kills")

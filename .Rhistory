# remove the observations with destroyed Top inhibitor but not Top Base Turret
filter(destroyedTopInhibitor == 0) %>%
filter(lostTopInhibitor == 0) %>%
# remove the observations with destroyed Mid inhibitor but not Mid Base Turret
filter(destroyedMidInhibitor == 0) %>%
filter(lostMidInhibitor == 0) %>%
# remove the variables with all 0s
select(!c(destroyedTopBaseTurret, destroyedMidBaseTurret, lostTopBaseTurret, lostMidBaseTurret, lostMidInhibitor, destroyedMidInhibitor, lostTopInhibitor, destroyedTopInhibitor))
# Preview the dataframe
as.tibble(lol.data)
lol.model <- lm(goldDiff~., data = lol.data)
summary(lol.model)
# Check multi-chollinearity
library(corrplot)
# Visualize the aliasing in the model matrix, excluding the intercept.
X <- model.matrix(~.-1, data = lol.data[,-1])
# Create color map on pairwise correlations.
contrast.vectors.correlations <- cor(X)
corrplot(contrast.vectors.correlations, type = "full", addgrid.col = "gray",
tl.col = "black", tl.srt = 90, method = "color", tl.cex=0.3)
# eliminate predictors
library(leaps)
Best_Subset <- regsubsets(goldDiff ~., data = lol.data,
nbest = 1, nvmax=NULL,
force.in = NULL, force.out=NULL,
method = "forward",
really.big=T)
summary_best_subset <- summary(Best_Subset)
# as.data.frame(summary_best_subset$outmat)
# let’s see what the package recommends in terms of the number of predictors to use for our dataset
which.max(summary_best_subset$adjr2)
# Summary table below provides details on which predictors to use for the model.
# The best predictors are indicated by ‘TRUE’.
# Select all predictors (remove the intercept)
predictor.list <- summary_best_subset$which[which.max(summary_best_subset$adjr2),][-1]
# Only keep the predictors are indicated by ‘TRUE’ and our response 'goldDiff'
keep <- names(predictor.list[predictor.list==T])
keep <- append(keep, c("goldDiff"))
# Using 'keep' as a mask to select best predictors in data set
new.lol.data <- lol.data[,(names(lol.data) %in% keep)]
# Preview the selected data set with only best predictors
as.tibble(new.lol.data)
lol.new.model <- lm(goldDiff~., data=new.lol.data)
summary(lol.new.model)
anova(lol.model, lol.new.model)
# Visualize the aliasing in the model matrix, excluding the intercept.
X <- model.matrix(~.-1, data = new.lol.data[,-1])
# Create color map on pairwise correlations.
contrast.vectors.correlations <- cor(X)
corrplot(contrast.vectors.correlations, type = "full", addgrid.col = "gray",
tl.col = "black", tl.srt = 90, method = "color", tl.cex=0.4)
library(car)
vif_values <- vif(lol.new.model)
vif_values
par(oma=c(0,1,0,0))
#create horizontal bar chart to display each VIF value
barplot(vif_values, main = "VIF Values", horiz = TRUE, col = "cyan", las=1, cex.names=0.4)
#add vertical line at 5
abline(v = 5, lwd = 3, lty = 2)
p <- plot(lol.new.model)
avPlot(lol.new.model, "kills")
drop <- c("expDiff")
new.lol.data[,!(names(new.lol.data) %in% drop)]
as.tibble(new.lol.data)
drop <- c("expDiff")
new.lol.data <- new.lol.data[,!(names(new.lol.data) %in% drop)]
as.tibble(new.lol.data)
lol.new.model <- lm(goldDiff~., data=new.lol.data)
summary(lol.new.model)
avPlot(lol.new.model, "assists")
# residual analysis -> identify bad high leverage points and outliers
p <- length(new.lol.data)
n <- length(new.lol.data$goldDiff)
high_leverage <- 2*p/n
plot(no_water_drake)
# residual analysis -> identify bad high leverage points and outliers
p <- length(new.lol.data)
n <- length(new.lol.data$goldDiff)
high_leverage <- 2*p/n
plot(lol.new.model)
abline(a =4, b=0)
abline(a =-4, b=0)
abline(v = high_leverage)
# residual analysis -> identify bad high leverage points and outliers
p <- length(new.lol.data)
n <- length(new.lol.data$goldDiff)
high_leverage <- 2*p/n
plot(lol.new.model)
abline(a =4, b=0, col = "red")
abline(a =-4, b=0, col = "red")
abline(v = high_leverage, col = "blue")
# residual analysis -> identify bad high leverage points and outliers
p <- length(new.lol.data)
n <- length(new.lol.data$goldDiff)
high_leverage <- 2*p/n
plot(lol.new.model)
abline(a =4, b=0, col = "red", lty = 2)
abline(a =-4, b=0, col = "red")
abline(v = high_leverage, col = "blue")
# residual analysis -> identify bad high leverage points and outliers
p <- length(new.lol.data)
n <- length(new.lol.data$goldDiff)
high_leverage <- 2*p/n
plot(lol.new.model)
abline(a =4, b=0, col = "red", lwd = 2)
abline(a =-4, b=0, col = "red", lwd = 2)
abline(v = high_leverage, col = "blue", lwd = 2)
avPlot(lol.new.model, "assists")
avPlot(lol.new.model, "wardsPlaced")
avPlot(lol.new.model, "lostWaterDrake")
avPlot(lol.new.model, "lostEarthDrake")
avPlot(lol.new.model, "killedFireDrake")
# Plots for potetial 'bad' predictors
avPlot(lol.new.model, "assists")
par(mfrow=c(2,2))
avPlot(lol.new.model, "wardsPlaced")
avPlot(lol.new.model, "lostWaterDrake")
avPlot(lol.new.model, "lostEarthDrake")
avPlot(lol.new.model, "killedFireDrake")
par(mfrow=c(2,2))
avPlots(lol.new.model)
knitr::opts_chunk$set(echo = TRUE)
# import data
lol.data <- read.csv("lol_games.csv")
# remove the column of game id and game duration
lol.data <- lol.data[,-1]
lol.data <- lol.data[,-1]
lol.model <- lm(goldDiff~., data = lol.data)
summary(lol.model)
# Since in summary of lol.model, we have all NA in these four predictors
# Thus, inspect the four columns
cat("Range of destroyedTopBaseTurret: ", range(lol.data$destroyedTopBaseTurret),
"\nRange of destroyedMidBaseTurret: ", range(lol.data$destroyedMidBaseTurret),
"\nRange of lostTopBaseTurret: ", range(lol.data$lostTopBaseTurret),
"\nRange of lostMidBaseTurret: ", range(lol.data$lostMidBaseTurret))
# Since all values in these four columns are 0, we can remove these predictors
# import tidyverse for Data Cleaning
library(tidyverse)
# If the Base Turret is not destroyed, then the Inhibitor cannot be destroyed
# Thus, the observations with Top/Mid Inhibitor > 0 but Top/Mid BaseTurret are invalid
# Therefore, we remove these observations and then remove these columns
# Also, BaseTurrent cannot be destroyed twice
# Thus, the observations with BotBaseTurret > 2 are invalid
# We Also need to remove these
lol.data <- lol.data %>%
# remove the observations with more than 1 Bottom Base Turrent being destroyed
filter(destroyedBotBaseTurret < 2) %>%
filter(lostBotBaseTurret < 2) %>%
# remove the observations with destroyed Top inhibitor but not Top Base Turret
filter(destroyedTopInhibitor == 0) %>%
filter(lostTopInhibitor == 0) %>%
# remove the observations with destroyed Mid inhibitor but not Mid Base Turret
filter(destroyedMidInhibitor == 0) %>%
filter(lostMidInhibitor == 0) %>%
# remove the variables with all 0s
select(!c(destroyedTopBaseTurret, destroyedMidBaseTurret, lostTopBaseTurret, lostMidBaseTurret, lostMidInhibitor, destroyedMidInhibitor, lostTopInhibitor, destroyedTopInhibitor))
# Preview the dataframe
as.tibble(lol.data)
lol.model <- lm(goldDiff~., data = lol.data)
summary(lol.model)
# Check multi-chollinearity
library(corrplot)
# Visualize the aliasing in the model matrix, excluding the intercept.
X <- model.matrix(~.-1, data = lol.data[,-1])
# Create color map on pairwise correlations.
contrast.vectors.correlations <- cor(X)
corrplot(contrast.vectors.correlations, type = "full", addgrid.col = "gray",
tl.col = "black", tl.srt = 90, method = "color", tl.cex=0.3)
# eliminate predictors
library(leaps)
Best_Subset <- regsubsets(goldDiff ~., data = lol.data,
nbest = 1, nvmax=NULL,
force.in = NULL, force.out=NULL,
method = "forward",
really.big=T)
summary_best_subset <- summary(Best_Subset)
# as.data.frame(summary_best_subset$outmat)
# let’s see what the package recommends in terms of the number of predictors to use for our dataset
which.max(summary_best_subset$adjr2)
# Summary table below provides details on which predictors to use for the model.
# The best predictors are indicated by ‘TRUE’.
# Select all predictors (remove the intercept)
predictor.list <- summary_best_subset$which[which.max(summary_best_subset$adjr2),][-1]
# Only keep the predictors are indicated by ‘TRUE’ and our response 'goldDiff'
keep <- names(predictor.list[predictor.list==T])
keep <- append(keep, c("goldDiff"))
# Using 'keep' as a mask to select best predictors in data set
new.lol.data <- lol.data[,(names(lol.data) %in% keep)]
# Preview the selected data set with only best predictors
as.tibble(new.lol.data)
lol.new.model <- lm(goldDiff~., data=new.lol.data)
summary(lol.new.model)
anova(lol.model, lol.new.model)
# Visualize the aliasing in the model matrix, excluding the intercept.
X <- model.matrix(~.-1, data = new.lol.data[,-1])
# Create color map on pairwise correlations.
contrast.vectors.correlations <- cor(X)
corrplot(contrast.vectors.correlations, type = "full", addgrid.col = "gray",
tl.col = "black", tl.srt = 90, method = "color", tl.cex=0.4)
# Using 'vif' function in 'car' package
library(car)
vif_values <- vif(lol.new.model)
# Set the left margin of plot be 1
par(oma=c(0,1,0,0))
#create horizontal bar chart to display each VIF value
barplot(vif_values, main = "VIF Values", horiz = TRUE, col = "cyan", las=1, cex.names=0.4)
#add vertical line at 5
abline(v = 5, lwd = 3, lty = 2)
# remove expDiff
drop <- c("expDiff")
new.lol.data <- new.lol.data[,!(names(new.lol.data) %in% drop)]
as.tibble(new.lol.data)
lol.new.model <- lm(goldDiff~., data=new.lol.data)
summary(lol.new.model)
# Using 'vif' function in 'car' package
library(car)
vif_values <- vif(lol.new.model)
# Set the left margin of plot be 1
par(oma=c(0,1,0,0))
#create horizontal bar chart to display each VIF value
barplot(vif_values, main = "VIF Values", horiz = TRUE, col = "cyan", las=1, cex.names=0.4)
#add vertical line at 5
abline(v = 5, lwd = 3, lty = 2)
# residual analysis -> identify bad high leverage points and outliers
p <- length(new.lol.data)
n <- length(new.lol.data$goldDiff)
high_leverage <- 2*p/n
# Display four plot at a time
par(mfrow=c(2,2 ))
plot(lol.new.model)
abline(a =4, b=0, col = "red", lwd = 2)
abline(a =-4, b=0, col = "red", lwd = 2)
abline(v = high_leverage, col = "blue", lwd = 2)
# Plots for potetial 'bad' predictors
avPlot(lol.new.model, "assists")
par(mfrow=c(2,2))
avPlot(lol.new.model, "wardsPlaced")
avPlot(lol.new.model, "lostWaterDrake")
avPlot(lol.new.model, "lostEarthDrake")
avPlot(lol.new.model, "killedFireDrake")
# Plots for some 'significant' predictors
par(mfrow=c(2,2))
avPlot(lol.new.model, c("champLevelDiff", "kills"))
# Plots for some 'significant' predictors
par(mfrow=c(2,2))
avPlot(lol.new.model, "champLevelDiff")
avPlot(lol.new.model, "destroyedBotBaseTurret")
avPlot(lol.new.model, "lostTopOuterTurret")
avPlot(lol.new.model, "kills")
knitr::opts_chunk$set(echo = TRUE)
hist(destroyedBotBaseTurret)
knitr::opts_chunk$set(echo = TRUE)
# import data
lol.data <- read.csv("lol_games.csv")
# remove the column of game id and game duration
lol.data <- lol.data[,-1]
lol.data <- lol.data[,-1]
lol.model.first <- lm(goldDiff~., data = lol.data)
summary(lol.model.first)
# Since in summary of lol.model, we have all NA in these four predictors
# Thus, inspect the four columns
cat("Range of destroyedTopBaseTurret: ", range(lol.data$destroyedTopBaseTurret),
"\nRange of destroyedMidBaseTurret: ", range(lol.data$destroyedMidBaseTurret),
"\nRange of lostTopBaseTurret: ", range(lol.data$lostTopBaseTurret),
"\nRange of lostMidBaseTurret: ", range(lol.data$lostMidBaseTurret))
# Since all values in these four columns are 0, we can remove these predictors
# import tidyverse for Data Cleaning
library(tidyverse)
# If the Base Turret is not destroyed, then the Inhibitor cannot be destroyed
# Thus, the observations with Top/Mid Inhibitor > 0 but Top/Mid BaseTurret are invalid
# Therefore, we remove these observations and then remove these columns
# Also, BaseTurrent cannot be destroyed twice
# Thus, the observations with BotBaseTurret > 2 are invalid
# We Also need to remove these
lol.data <- lol.data %>%
# remove the observations with more than 1 Bottom Base Turrent being destroyed
filter(destroyedBotBaseTurret < 2) %>%
filter(lostBotBaseTurret < 2) %>%
# remove the observations with destroyed Top inhibitor but not Top Base Turret
filter(destroyedTopInhibitor == 0) %>%
filter(lostTopInhibitor == 0) %>%
# remove the observations with destroyed Mid inhibitor but not Mid Base Turret
filter(destroyedMidInhibitor == 0) %>%
filter(lostMidInhibitor == 0) %>%
# remove the variables with all 0s
select(!c(destroyedTopBaseTurret, destroyedMidBaseTurret,
lostTopBaseTurret, lostMidBaseTurret, lostMidInhibitor,
destroyedMidInhibitor, lostTopInhibitor, destroyedTopInhibitor))
hist(destroyedBotBaseTurret)
hist(lol.data$destroyedBotBaseTurret)
knitr::opts_chunk$set(echo = TRUE)
# import data
lol.data <- read.csv("lol_games.csv")
# remove the column of game id and game duration
lol.data <- lol.data[,-1]
lol.data <- lol.data[,-1]
lol.model.first <- lm(goldDiff~., data = lol.data)
summary(lol.model.first)
# Since in summary of lol.model, we have all NA in these four predictors
# Thus, inspect the four columns
cat("Range of destroyedTopBaseTurret: ", range(lol.data$destroyedTopBaseTurret),
"\nRange of destroyedMidBaseTurret: ", range(lol.data$destroyedMidBaseTurret),
"\nRange of lostTopBaseTurret: ", range(lol.data$lostTopBaseTurret),
"\nRange of lostMidBaseTurret: ", range(lol.data$lostMidBaseTurret))
# Since all values in these four columns are 0, we can remove these predictors
hist(lol.data$destroyedBotBaseTurret)
hist(lol.data$destroyedBotBaseTurret, col = "red")
hist(lol.data$destroyedBotBaseTurret, col = c("gray", "red")
hist(lol.data$destroyedBotBaseTurret, col = c("gray","gray", "gray", "red"))
hist(lol.data$destroyedBotBaseTurret, col = c("gray","gray", "gray", "red"), breaks = 4)
hist(lol.data$destroyedBotBaseTurret, col = c("gray","gray", "gray", "red"), breaks = 4, xlab = "Destroyed Bot Base Turret", main = "Distribution of value in destroyedBotBaseTurret")
View(lol.model.first)
hist(lol.data$lostBotBaseTurret, col = c("gray","gray", "gray", "red"), breaks = 4, xlab = "Lost Bot Base Turret", main = "Distribution of value in lostBotBaseTurret")
knitr::opts_chunk$set(echo = TRUE)
# import data
lol.data <- read.csv("lol_games.csv")
# remove the column of game id and game duration
lol.data <- lol.data[,-1]
lol.data <- lol.data[,-1]
lol.model.first <- lm(goldDiff~., data = lol.data)
summary(lol.model.first)
# Since in summary of lol.model, we have all NA in these four predictors
# Thus, inspect the four columns
cat("Range of destroyedTopBaseTurret: ", range(lol.data$destroyedTopBaseTurret),
"\nRange of destroyedMidBaseTurret: ", range(lol.data$destroyedMidBaseTurret),
"\nRange of lostTopBaseTurret: ", range(lol.data$lostTopBaseTurret),
"\nRange of lostMidBaseTurret: ", range(lol.data$lostMidBaseTurret))
# Since all values in these four columns are 0, we can remove these predictors
hist(lol.data$lostBotBaseTurret, col = c("gray","gray", "gray", "red"), breaks = 4, xlab = "Lost Bot Base Turret", main = "Distribution of value in lostBotBaseTurret")
# import tidyverse for Data Cleaning
library(tidyverse)
# If the Base Turret is not destroyed, then the Inhibitor cannot be destroyed
# Thus, the observations with Top/Mid Inhibitor > 0 but Top/Mid BaseTurret are invalid
# Therefore, we remove these observations and then remove these columns
# Also, BaseTurrent cannot be destroyed twice
# Thus, the observations with BotBaseTurret > 2 are invalid
# We Also need to remove these
lol.data <- lol.data %>%
# remove the observations with more than 1 Bottom Base Turrent being destroyed
filter(destroyedBotBaseTurret < 2) %>%
filter(lostBotBaseTurret < 2) %>%
# remove the observations with destroyed Top inhibitor but not Top Base Turret
filter(destroyedTopInhibitor == 0) %>%
filter(lostTopInhibitor == 0) %>%
# remove the observations with destroyed Mid inhibitor but not Mid Base Turret
filter(destroyedMidInhibitor == 0) %>%
filter(lostMidInhibitor == 0) %>%
# remove the variables with all 0s
select(!c(destroyedTopBaseTurret, destroyedMidBaseTurret,
lostTopBaseTurret, lostMidBaseTurret, lostMidInhibitor,
destroyedMidInhibitor, lostTopInhibitor, destroyedTopInhibitor))
# Preview the dataframe
as.tibble(lol.data)
lol.model.second <- lm(goldDiff~., data = lol.data)
summary(lol.model.second)
# Check multi-chollinearity
library(corrplot)
# Visualize the aliasing in the model matrix, excluding the intercept.
X <- model.matrix(~.-1, data = lol.data[,-1])
# Create color map on pairwise correlations.
contrast.vectors.correlations <- cor(X)
corrplot(contrast.vectors.correlations, type = "full", addgrid.col = "gray",
tl.col = "black", tl.srt = 90, method = "color", tl.cex=0.3)
# Using 'vif' function in 'car' package
library(car)
vif_values <- vif(lol.model.second)
# Set the left margin of plot be 1
par(oma=c(0,1,0,0))
#create horizontal bar chart to display each VIF value
barplot(vif_values, main = "VIF Values", horiz = TRUE, col = "cyan", las=1, cex.names=0.4)
#add vertical line at 5
abline(v = 5, lwd = 3, lty = 2)
# remove expDiff
drop <- c("expDiff")
lol.data <- lol.data[,!(names(lol.data) %in% drop)]
as.tibble(lol.data)
lol.model.second <- lm(goldDiff~., data = lol.data)
summary(lol.model.second)
# To avoid those variables with larger ranges dominating over those with small ranges
# We standardize assist and kills by z-score = (value - mean) / standard deviation
par(mfrow=c(3,1))
hist(lol.data$assists, col = "red", breaks = 14,
xlim = c(0, 140), ylim = c(0, 4000),
xlab = "Assists", main = "Assists Distribution")
hist(lol.data$kills, col="blue", breaks = 7,
xlim = c(0, 140),  ylim = c(0, 4000),
xlab = "Kills", main = "Kills Distribution")
hist(lol.data$deaths, col="green", breaks = 7,
xlim = c(0, 140),  ylim = c(0, 4000),
xlab = "Deaths", main = "Deaths Distribution")
# We can find after modification, the mean and variance of kills and assists are the same,
# as well as their ranges
par(mfrow=c(3,1))
# lol.data$kills = (lol.data$kills - mean(lol.data$kills)) / sd(lol.data$kills)
lol.data$assists = ((lol.data$assists - mean(lol.data$assists)) / sd(lol.data$assists)
* sd(lol.data$kills) + mean(lol.data$kills))
# lol.data$deaths = (lol.data$deaths - mean(lol.data$deaths)) / sd(lol.data$deaths)
boxplot(lol.data$assists, col="red", horizontal = T,
ylim = c(0, 90),
xlab = "Modified Assists", main = "Standardized Assists Distribution")
boxplot(lol.data$kills, col="blue", horizontal = T, ylim = c(0, 90),
xlab = "Modified Kills", main = "Standardized Kills Distribution")
boxplot(lol.data$deaths, col="green", horizontal = T, ylim = c(0, 90),
xlab = "Modified Deaths", main = "Standardized Deaths Distribution")
# Then, we can start to calculate KDA
# lapply(lol.data, function(x){ length(which(x==0))}) # Check the number of zeros in each column
# Set those death = 0 to death = 1, since that's how KDA is calculated
lol.data$deaths[lol.data$deaths == 0] <- 1
lol.data$KDA <- (lol.data$kills + lol.data$assists) / lol.data$deaths
# Remove Kills, Assists, and Deaths
lol.data <- lol.data %>%
select(!c(kills, assists, deaths))
# Perview the data frame
as.tibble(lol.data)
lol.model.third <- lm(goldDiff~., data=lol.data)
summary(lol.model.third)
# Using 'vif' function in 'car' package
library(car)
vif_values <- vif(lol.model.third)
# Set the left margin of plot be 1
par(oma=c(0,1,0,0))
#create horizontal bar chart to display each VIF value
barplot(vif_values, main = "VIF Values",
horiz = TRUE, col = "cyan", las=1, cex.names=0.4, xlim = c(0, 6))
#add vertical line at 5
abline(v = 5, lwd = 3, lty = 2)
# Check multi-chollinearity
library(corrplot)
# Visualize the aliasing in the model matrix, excluding the intercept.
X <- model.matrix(~.-1, data = lol.data[,-1])
# Create color map on pairwise correlations.
contrast.vectors.correlations <- cor(X)
corrplot(contrast.vectors.correlations, type = "full", addgrid.col = "gray",
tl.col = "black", tl.srt = 90, method = "color", tl.cex=0.3)
contrast.vectors.correlations[c(15, 16), c(15, 16)]
# There're other correlated predictors such as KDA and champLDiff,
# destroyedBotNexusTurret and destroyedBotInhibitor,
# lostBotNexusTurret and lostBotInhibitor
contrast.vectors.correlations[abs(contrast.vectors.correlations) > 0.6]
# Remove lostRiftHerald
lol.data <- lol.data %>%
select(!c(lostRiftHerald))
# Preview the Data Set
as.tibble(lol.data)
# eliminate predictors
library(leaps)
Best_Subset <- regsubsets(goldDiff ~., data = lol.data,
nbest = 1, nvmax=NULL,
force.in = NULL, force.out=NULL,
method = "exhaustive",
really.big=F)
summary_best_subset <- summary(Best_Subset)
# as.data.frame(summary_best_subset$outmat)
# R square will always increase if one predictor is added
plot(summary_best_subset$rsq, xlab = "Number of predictors", ylab = "R^2",
type = "l", col = "blue", main = 'Best Subset Selection')
plot(summary_best_subset$rss, xlab = "Number of predictors", ylab = "RSS",
type = "l", col = "blue", main = 'Best Subset Selection')
# Plot RSS
plot(summary_best_subset$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
# Plot Adjusted R^2, highlight max value
plot(summary_best_subset$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")
max = which.max(summary_best_subset$adjr2)
points(max, summary_best_subset$adjr2[max], col = "red", cex = 2, pch = 20)
# Plot BIC, highlight min value
plot(summary_best_subset$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
min = which.min(summary_best_subset$bic)
points(min, summary_best_subset$bic[min], col = "red", cex = 2, pch = 20)
# We first select the best model according to BIC.
modelwith.minimum.BIC <- which.min(summary_best_subset$bic)
best.model <- summary_best_subset$which[modelwith.minimum.BIC,][-1]
# Only keep the predictors are indicated by ‘TRUE’ and our response 'goldDiff'
keep <- names(best.model[best.model==T])
keep <- append(keep, c("goldDiff"))
# Using 'keep' as a mask to select best predictors in data set
lol.data <- lol.data[,(names(lol.data) %in% keep)]
# Preview the selected data set with only best predictors
as.tibble(lol.data)
lol.model.forth <- lm(goldDiff~., data=lol.data)
# T-test or Partial F-test
summary(lol.model.forth)
# Using 'vif' function in 'car' package
library(car)
vif_values <- vif(lol.model.forth)
# Set the left margin of plot be 1
par(oma=c(0,1,0,0))
#create horizontal bar chart to display each VIF value
barplot(vif_values, main = "VIF Values",
horiz = TRUE, col = "cyan", las=1, cex.names=0.4, xlim = c(0, 6))
#add vertical line at 5
abline(v = 5, lwd = 3, lty = 2)
# Visualize the aliasing in the model matrix, excluding the intercept.
X <- model.matrix(~.-1, data = lol.data[,-1])
# Create color map on pairwise correlations.
contrast.vectors.correlations <- cor(X)
corrplot(contrast.vectors.correlations, type = "full", addgrid.col = "gray",
tl.col = "black", tl.srt = 90, method = "color", tl.cex=0.4)
# number of coefficient in our model, including the intercept
# The sum of diagonal of leverage matrix
p <- length(lol.data)
n <- length(lol.data$goldDiff) # Number of observations in our model
# a high leverage point is the point with more than twice of the average leverage score
high_leverage <- 2*p/n
# Display four plot at a time
par(mfrow=c(2,2 ))
plot(lol.model.forth)
# Since our dataset can be considered as 'large', according to Campuswire post
# High leverage points with standardized residual outside of (-4, 4) are considered bad
abline(a =4, b=0, col = "red", lwd = 2)
abline(a =-4, b=0, col = "red", lwd = 2)
abline(v = high_leverage, col = "blue", lwd = 2)
summary(lol.data)
